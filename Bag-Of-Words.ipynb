{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b36ac61",
   "metadata": {},
   "source": [
    "### BAG OF WORDS\n",
    "Bag of Words (BoW) is an algorithm that counts the number of times a word appears in a document. The word counts are then used to compare documents for similarities and subsequent classification and topic modeling. It is a way to prepare text before inputing it in a deep learning net.\n",
    "The frequency of words is converted to a probabilities and when these probabilities pass certain levels nodes are activated within a network and thereby giving us classification of the document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6006d10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\MUKIRI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\MUKIRI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\MUKIRI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a67674e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"Thank you all so very much. Thank you to the Academy. \n",
    "               Thank you to all of you in this room. I have to congratulate \n",
    "               the other incredible nominees this year. The Revenant was \n",
    "               the product of the tireless efforts of an unbelievable cast\n",
    "               and crew. First off, to my brother in this endeavor, Mr. Tom \n",
    "               Hardy. Tom, your talent on screen can only be surpassed by \n",
    "               your friendship off screen … thank you for creating a t\n",
    "               ranscendent cinematic experience. Thank you to everybody at \n",
    "               Fox and New Regency … my entire team. I have to thank \n",
    "               everyone from the very onset of my career … To my parents; \n",
    "               none of this would be possible without you. And to my \n",
    "               friends, I love you dearly; you know who you are. And lastly,\n",
    "               I just want to say this: Making The Revenant was about\n",
    "               man's relationship to the natural world. A world that we\n",
    "               collectively felt in 2015 as the hottest year in recorded\n",
    "               history. Our production needed to move to the southern\n",
    "               tip of this planet just to be able to find snow. Climate\n",
    "               change is real, it is happening right now. It is the most\n",
    "               urgent threat facing our entire species, and we need to work\n",
    "               collectively together and stop procrastinating. We need to\n",
    "               support leaders around the world who do not speak for the \n",
    "               big polluters, but who speak for all of humanity, for the\n",
    "               indigenous people of the world, for the billions and \n",
    "               billions of underprivileged people out there who would be\n",
    "               most affected by this. For our children’s children, and \n",
    "               for those people out there whose voices have been drowned\n",
    "               out by the politics of greed. I thank you all for this \n",
    "               amazing award tonight. Let us not take this planet for \n",
    "               granted. I do not take tonight for granted. Thank you so very much.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52cf33e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "wn = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7eab9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f0b571b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thank you all so very much.', 'Thank you to the Academy.', 'Thank you to all of you in this room.', 'I have to congratulate \\n               the other incredible nominees this year.', 'The Revenant was \\n               the product of the tireless efforts of an unbelievable cast\\n               and crew.', 'First off, to my brother in this endeavor, Mr. Tom \\n               Hardy.', 'Tom, your talent on screen can only be surpassed by \\n               your friendship off screen … thank you for creating a t\\n               ranscendent cinematic experience.', 'Thank you to everybody at \\n               Fox and New Regency … my entire team.', 'I have to thank \\n               everyone from the very onset of my career … To my parents; \\n               none of this would be possible without you.', 'And to my \\n               friends, I love you dearly; you know who you are.', \"And lastly,\\n               I just want to say this: Making The Revenant was about\\n               man's relationship to the natural world.\", 'A world that we\\n               collectively felt in 2015 as the hottest year in recorded\\n               history.', 'Our production needed to move to the southern\\n               tip of this planet just to be able to find snow.', 'Climate\\n               change is real, it is happening right now.', 'It is the most\\n               urgent threat facing our entire species, and we need to work\\n               collectively together and stop procrastinating.', 'We need to\\n               support leaders around the world who do not speak for the \\n               big polluters, but who speak for all of humanity, for the\\n               indigenous people of the world, for the billions and \\n               billions of underprivileged people out there who would be\\n               most affected by this.', 'For our children’s children, and \\n               for those people out there whose voices have been drowned\\n               out by the politics of greed.', 'I thank you all for this \\n               amazing award tonight.', 'Let us not take this planet for \\n               granted.', 'I do not take tonight for granted.', 'Thank you so very much.']\n"
     ]
    }
   ],
   "source": [
    "print (sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9952aefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To store our sentences after cleaning \n",
    "corpus = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bd800a",
   "metadata": {},
   "source": [
    "### Cleaning the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7d4c184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing commas, full stops, punctuation marks \n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i]) #apart from words a to z remove everything with spaces and do this to all sentences\n",
    "    review = review.lower() #make lower case all words for all sentences\n",
    "    review = review.split() #to get a list of words\n",
    "    #############################################################################\n",
    "    # TRY BOTH STEMMING AND LEMMATIZATION\n",
    "    \n",
    "    #review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))] # stemming\n",
    "    \n",
    "    review = [wn.lemmatize(word) for word in review if word not in set(stopwords.words('english'))]\n",
    "    \n",
    "    review = ''.join(review) #join the list of words in review to review\n",
    "    corpus.append(review) # append to our corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "655577f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thankmuch', 'thankacademy', 'thankroom', 'congratulateincrediblenomineeyear', 'revenantproducttirelesseffortunbelievablecastcrew', 'firstbrotherendeavormrtomhardy', 'tomtalentscreensurpassedfriendshipscreenthankcreatingranscendentcinematicexperience', 'thankeverybodyfoxnewregencyentireteam', 'thankeveryoneonsetcareerparentnonewouldpossiblewithout', 'friendlovedearlyknow', 'lastlywantsaymakingrevenantmanrelationshipnaturalworld', 'worldcollectivelyfelthottestyearrecordedhistory', 'productionneededmovesoutherntipplanetablefindsnow', 'climatechangerealhappeningright', 'urgentthreatfacingentirespecieneedworkcollectivelytogetherstopprocrastinating', 'needsupportleaderaroundworldspeakbigpolluterspeakhumanityindigenouspeopleworldbillionbillionunderprivilegedpeoplewouldaffected', 'childchildpeoplewhosevoicedrownedpoliticsgreed', 'thankamazingawardtonight', 'letutakeplanetgranted', 'taketonightgranted', 'thankmuch']\n"
     ]
    }
   ],
   "source": [
    "#Note that with stemming some words lose meaning \n",
    "#lemmatization accomplishes this in a much better way\n",
    "\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910205b8",
   "metadata": {},
   "source": [
    "### Creating the BOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e068de58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "X = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df7b4625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebc36ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
